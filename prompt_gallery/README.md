# ğŸ§  Prompt Gallery (WIP)

A work-in-progress project exploring how different LLMs behave across handcrafted prompts designed for diverse tasks.

## ğŸ§¾ Overview

This project focuses on:
- Designing structured prompts by categories
- Running them through multiple free-access LLMs
- Comparing outputs for insights into reasoning, verbosity, tone, etc.

## ğŸ“ Structure

```
llm-playground/
â””â”€â”€ prompt_gallery/             # ğŸ§  Prompt testing & evaluation module
    â”œâ”€â”€ prompts/                # ğŸ“ Handcrafted prompts (.jsonl)
    â”œâ”€â”€ scripts/                # âš™ï¸ Scripts to query LLM APIs
    â”œâ”€â”€ results/                # ğŸ“Š Model outputs  
    â”œâ”€â”€ notebooks/              # ğŸ““ Jupyter notebooks for analysis & comparison
    â””â”€â”€ README.md               # ğŸ“„ Project documentation
```


## ğŸš§ Current Status

- [x] Project folder initialized
- [ ] Initial prompt set in JSONL
- [ ] Hugging Face script for model calls
- [ ] Output comparison notebook

## ğŸ”œ Next Steps

- Write `prompts.jsonl` with 20+ curated entries
- Run prompts through free models via Hugging Face API
- Create a notebook to compare outputs and annotate findings