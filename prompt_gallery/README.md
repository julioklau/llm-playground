# ğŸ§  Prompt Gallery

This project explores how different large language models (LLMs) respond to structured prompts across varied task categories. It enables automated prompt testing and output collection using multiple free-access models.

## ğŸš€ Highlights

- Compare outputs of different LLMs using structured, category-based prompts
- Automate prompt runs across multiple free-access APIs (Groq)
- Score model responses using an LLM-based judge for qualitative evaluation
- Output everything to JSONL and logs for reproducibility and future benchmarking
- Modular folder structure, easy to extend with new models or prompt sets

## ğŸ§¾ Overview

This project focuses on:
- Designing structured prompts by categories
- Running them through multiple free-access LLMs
- Comparing outputs for insights into reasoning, verbosity, tone, etc.
- Scoring the outputs using an automated LLM-based judge

## ğŸ“ Structure

```
llm-playground/
â””â”€â”€ prompt_gallery/             # ğŸ§  Prompt testing & evaluation module
    â”œâ”€â”€ config/                 # ğŸ¤– Model config and wrappers
    â”œâ”€â”€ logs/                   # ğŸ“‚ Runtime logs (ignored by Git)
    â”œâ”€â”€ notebooks/              # ğŸ““ Analysis notebooks to compare outputs    
    â”œâ”€â”€ outputs/                # ğŸ“Š Model outputs and scores
    â”œâ”€â”€ prompts/                # ğŸ“ Prompt sets (.jsonl)
    â”œâ”€â”€ scripts/                # âš™ï¸ Scripts to run prompts and call models
    â”œâ”€â”€ utils/                  # ğŸ”§ Helpers and shared utilities 
    â””â”€â”€ README.md               # ğŸ“„ Project documentation
```

## ğŸ”§ Tech Stack

- **Python 3.12** â€“ Core language
- **LLM APIs** â€“ Groq (Gemma 7B, LLaMA 3 8B, LLaMA 3 70B)
- **dotenv** â€“ For managing `.env` configuration
- **Jupyter Notebooks** â€“ For qualitative analysis of outputs
- **Logging** â€“ Runtime information and prompt traceability
- **pipreqs** â€“ For auto-generating `requirements.txt`
- **JSONL** â€“ Structured prompt and response formats

## âš™ï¸ Setup

This project uses a `.env` file for managing paths and API keys.


### ğŸ”‘ Step 1: Get you Groq API Key
1. Visit [https://console.groq.com/keys](https://console.groq.com/keys)
2. Log in or sign up (use Google or email).
3. Click **"Create API Key"**
4. Copy and paste the generated key

### ğŸ§ª Step 2: Set Up the `.env` File
Copy the example file name as .env.example:
```bash
cp .env.example .env
```
Fill in values:
```env
GROQ_API_KEY=your_groq_api_key_here
PROMPT_PATH=prompts/prompts.jsonl
CONFIG_PATH=config
OUTPUT_PATH=outputs
LOG_PATH=logs
```
### ğŸ“¦ Step 3: Installation

Make sure youâ€™re using Python 3.8+ (preferably 3.12). Then, install dependencies using pip:
```bash
pip install -r requirements.txt
```

### ğŸš€ Step 4: Usage
Run all prompts and automatically evaluate responses using an LLM-based judge:
```bash
python -m scripts.main
```

## ğŸš§ Current Status

- [x] Project folder initialized
- [x] Initial prompt set in JSONL
- [x] Prompt runner script connected to two LLMs
- [x] Output generation working
- [x] Output comparison notebook
- [x] LLM judge script to evaluate outputs
- [x] Structured prompt pipeline
- [x] Logging, output, and evaluation setup

## ğŸ¯ Goals

- Understand how models differ in reasoning and expression
- Identify patterns in hallucination, verbosity, and reasoning style
- Gain insights into model-specific response behaviors and styles

## ğŸ“„ License
This project is covered under the [Apache 2.0 License](../LICENSE), as specified in the root of this repository.

## ğŸ¤ Contact

Created by [julioklau](https://github.com/julioklau) â€” feel free to reach out if you have questions or feedback!
- GitHub: [julioklau](https://github.com/julioklau)
- LinkedIn: [Julio Lau](https://linkedin.com/in/julio-lau)
- Email: julioklau97@gmail.com