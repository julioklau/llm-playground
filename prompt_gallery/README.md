# ğŸ§  Prompt Gallery (WIP)

This project explores how different large language models (LLMs) respond to structured prompts across varied task categories. It enables automated prompt testing and output collection using multiple free-access models.

## ğŸ§¾ Overview

This project focuses on:
- Designing structured prompts by categories
- Running them through multiple free-access LLMs
- Comparing outputs for insights into reasoning, verbosity, tone, etc.

## ğŸ“ Structure

```
llm-playground/
â””â”€â”€ prompt_gallery/             # ğŸ§  Prompt testing & evaluation module
    â”œâ”€â”€ logs/                   # ğŸ“‚ Logs of model responses and runtime info (ignored by Git, generated at runtime)
    â”œâ”€â”€ models/                 # ğŸ¤– Model wrappers or config files
    â”œâ”€â”€ notebooks/              # ğŸ““ Analysis notebooks to compare outputs
    â”œâ”€â”€ outputs/                # ğŸ“Š Generated model outputs per prompt
    â”œâ”€â”€ prompts/                # ğŸ“ Curated prompt sets (.jsonl)
    â”œâ”€â”€ scripts/                # âš™ï¸ Scripts to run prompts and call models
    â””â”€â”€ README.md               # ğŸ“„ Project documentation
```
## âš™ï¸ Configuration

This project uses a `.env` file to manage environment variables (API keys, model settings, output paths, etc.).


### ğŸ”‘ Step 1: Get you groq API Key
1. Visit [https://console.groq.com/keys](https://console.groq.com/keys)
2. Log in or sign up (use Google or email).
3. Click **"Create API Key"**
4. Copy and paste the generated key

### ğŸ§ª Step 2: Set Up the `.env` File
1. Copy the example file name as .env.example:
   ```bash
   cp .env.example .env
2. Fill in your actual values:
    ```env
    GROQ_API_KEY=your_groq_api_key_here
    PROMPT_PATH=prompts/prompts.jsonl
    MODEL_CONFIG_PATH=models/models.json
    OUTPUT_PATH=outputs
    LOG_PATH=logs/run_prompts.log
## ğŸš§ Current Status

- [x] Project folder initialized
- [x] Initial prompt set in JSONL
- [x] Prompt runner script connected to two LLMs
- [x] Output generation working
- [ ] Output comparison notebook

## ğŸ”œ Next Steps

- Develop an evaluation notebook to compare output structure and tone
- Explore qualitative metrics and annotation tagging

## ğŸ¯ Goals

- Understand how different models interpret the same task
- Identify patterns in hallucination, verbosity, and reasoning style